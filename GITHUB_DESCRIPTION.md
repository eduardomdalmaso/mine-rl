# MineRL Agent Training

Um ambiente Python para treinar agentes de **Reinforcement Learning (RL)** em ambientes complexos, comeÃ§ando com Gymnasium e expandindo para Minecraft.

## ğŸ¯ Objetivo

Este projeto fornece um setup pronto para praticar e desenvolver algoritmos de RL, evoluindo de problemas simples (CartPole) para ambientes mais desafiadores como Minecraft. Ideal para quem quer aprender RL de forma progressiva.

## ğŸš€ ComeÃ§ando

### PrÃ©-requisitos
- Python 3.12+
- pip

### InstalaÃ§Ã£o RÃ¡pida

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/minerl-rl-agent.git
cd minerl-rl-agent

# Execute o setup automÃ¡tico
bash scripts/setup.sh

# Ou configure manualmente
python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Primeiro Teste

```bash
source venv/bin/activate
python test_env.py
```

## ğŸ“¦ DependÃªncias

- **gymnasium**: Ambientes RL padronizados
- **stable-baselines3**: Algoritmos RL prontos (PPO, DQN, A2C, etc)
- **torch**: Deep Learning framework
- **numpy**: ComputaÃ§Ã£o numÃ©rica
- **opencv-python**: Processamento de imagens

## ğŸ“š Estrutura do Projeto

```
.
â”œâ”€â”€ agent_basic.py          # Exemplo bÃ¡sico de agente PPO
â”œâ”€â”€ test_env.py             # Script de teste do ambiente
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ README.md              # DocumentaÃ§Ã£o detalhada
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup.sh           # Script de configuraÃ§Ã£o automÃ¡tica
â””â”€â”€ venv/                  # Ambiente virtual Python
```

## ğŸ’¡ Exemplos de Uso

### Teste o ambiente
```bash
python test_env.py
```

### Treine um agente
```python
from agent_basic import create_agent, train_agent, test_agent

model, env = create_agent()
train_agent(model, env, timesteps=50000)
test_agent(model, env, episodes=5)
model.save("ppo_cartpole")
env.close()
```

### Use um modelo treinado
```python
from stable_baselines3 import PPO
import gymnasium

env = gymnasium.make('CartPole-v1')
model = PPO.load("ppo_cartpole")

obs, _ = env.reset()
done = False
while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    env.render()
env.close()
```

## ğŸ® Ambientes DisponÃ­veis

### NÃ­vel 1: Simples
- `CartPole-v1` - Balancear um poste em um carrinho
- `MountainCar-v0` - Subir uma montanha com um carro

### NÃ­vel 2: IntermediÃ¡rio
- `Pendulum-v1` - Controlar um pÃªndulo
- `LunarLander-v2` - Pousar um mÃ³dulo lunar
- `BipedalWalker-v3` - Fazer um bÃ­pede caminhar

### NÃ­vel 3: AvanÃ§ado
- `Minecraft` (com MineRL) - Ambientes 3D complexos
- `Atari` (com ALE) - Jogos clÃ¡ssicos

## ğŸ¤– Algoritmos Suportados

- **PPO** (Proximal Policy Optimization) - Recomendado para iniciantes
- **DQN** (Deep Q-Network)
- **A2C** (Advantage Actor-Critic)
- **DDPG** (Deep Deterministic Policy Gradient)
- **SAC** (Soft Actor-Critic)
- **TD3** (Twin Delayed DDPG)

## ğŸ“ˆ Roadmap

- [x] Setup bÃ¡sico com Gymnasium
- [x] Exemplos com PPO
- [x] Script de teste
- [ ] Suporte a Minecraft (MineRL)
- [ ] Treinamento com mÃºltiplos ambientes
- [ ] VisualizaÃ§Ã£o de treinamento com TensorBoard
- [ ] Exemplos com transfer learning
- [ ] DocumentaÃ§Ã£o expandida

## ğŸ”— Recursos Ãšteis

- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- [MineRL](https://minerl.io/)
- [OpenAI Spinning Up](https://spinningup.openai.com/)

## ğŸ“ LicenÃ§a

MIT License - veja LICENSE.md para mais detalhes

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se livre para abrir issues e pull requests.

## âœï¸ Autor

[Seu Nome]

## ğŸ“§ Contato

Para dÃºvidas ou sugestÃµes, abra uma issue no repositÃ³rio.

---

**â­ Se este projeto foi Ãºtil, considere dar uma estrela!**
